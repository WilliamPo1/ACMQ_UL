---
output:
  xaringan::moon_reader:
    css: ["lib/fonts.css", "lib/slides.css"]   
    lib_dir: lib
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false
---

class: title
background-image: url(fig/wall_tomato.jpg)
background-size: cover

# ACMQ Ch.21 Statistiques

---

.massive[Opérateurs: espérance, variance, covariance]

---

# Espérance

L'espérance mathématique représente la valeur qui serait produite, en moyenne, si on répétait une même procédure ou une même expérience un très grand nombre de fois. 

On peut manipuler l'espérance en appliquant trois règles. 

Si $a$ représente une constante, et que $X$ et $Y$ représentent des variables aléatoires, alors :

$$\begin{align*}
E[a] = a \hspace{1in} &\mbox{Règle 1} \\
E[X+Y] = E[X] + E[Y] \hspace{1in} &\mbox{Règle 2} \\
E[a \cdot X] = a \cdot E[X] \hspace{1in} &\mbox{Règle 3}
\end{align*}$$

L'intuition derrière la règle 1 est facile à saisir: si un phénomène génère toujours le même résultat (une constante $a$) alors l'espérance du phénomène est égale à $a$. Par exemple, $E[2]=2$.

Les règles 2 et 3 indiquent que l'espérance d'une somme est égale à la somme des espérances, et que l'espérance du produit d'une constante et d'une variable est égale au produit de la constante et de l'espérance de la variable.

---

# Variance

Des règles similaires permettent de manipuler la variance:

$$\begin{align}
\mbox{Var}(a)=0 \hspace{1in} &\mbox{Règle 1} \\
\mbox{Var}(a + X) = \mbox{Var}(X) \hspace{1in} &\mbox{Règle 2} \\
\mbox{Var}(aX)=a^2 \mbox{Var}(X) \hspace{1in} &\mbox{Règle 3} \\
\mbox{Var}(X+Y) = \mbox{Var}(X)+\mbox{Var}(Y)+2\mbox{Cov}(X,Y) \hspace{1in} &\mbox{Règle 4} 
\end{align}$$

L'intuition derrière la règle 1 est simple: si un phénomène génère toujours le même résultat (une constante $a$) alors par définition la variance est nulle. 

La véracité des autres règles est facile à démontrer, même si l'intuition est moins directe.

---

# Covariance

Les covariances peuvent également être transformées et manipulées:

$$\begin{align*}
\mbox{Cov}(a, X) = 0 \hspace{1in} &\mbox{Règle 1}\\
\mbox{Cov}(X, X) = \mbox{Var}(X)\hspace{1in} &\mbox{Règle 2}\\
\mbox{Cov}(aX, Y) = a\mbox{Cov}(X, Y) \hspace{1in} &\mbox{Règle 3}\\
\mbox{Cov}(X+Y, Z) = \mbox{Cov}(X,Z) + \mbox{Cov}(Y,Z) \hspace{1in} &\mbox{Règle 4}
\end{align*}$$

---

.massive[Propriétés de la moyenne d'un échantillon]

---

# Biais de la moyenne d'un échantillon

> Un analyste s'intéresse à une population, mais il n'a pas les ressources pour observer tous ses membres. Afin d'estimer la moyenne de la population, il pige un échantillon aléatoire simple de taille $n$: 

> $$X=\{X_1, X_2, X_3, ..., X_n\}$$ 

> Il calcule la moyenne arithmétique de cet échantillon:

> $$\bar{X} = \frac{1}{n} (X_1 + X_2 + ... + X_n)$$

Est-ce que la moyenne calculée à partir de cet échantillon est un bon estimé de la moyenne de la population? 

Imaginons que l'analyste tire un très grand nombre d'échantillons distincts, et qu'il calcule une moyenne distincte dans chaque échantillon. 

L'estimateur est non-biaisé si, en moyenne, il produit la bonne réponse : la moyenne de l'échantillon est estimateur non-biaisé de la moyenne de la population si $E[\bar{X}]=E[X]$.

---

# Biais de la moyenne d'un échantillon

Pour établir cette égalité, nous commençons par prendre l'espérance de la moyenne échantillonnale:

$$E[\bar{X}] = E\left [ \frac{1}{n} (X_1 + X_2 + ... + X_n) \right ]$$

Ensuite, nous manipulons l'espérance à l'aide des règles introduites dans la section précédente:

$$\begin{align*}
E[\bar{X}] &= \frac{1}{n} E\left [(X_1 + X_2 + ... + X_n) \right ] && \mbox{(Règle 3)} \\
           &= \frac{1}{n} \left (E[X_1] + E[X_2] + ... + E[X_n] \right ) && \mbox{(Règle 2)}
\end{align*}$$

---

# Biais de la moyenne d'un échantillon

La moyenne est calculée à partir d'un échantillon aléatoire simple : les variables $X_1$ à $X_n$ ont la même espérance:

$$\begin{align*}
E[\bar{X}] &= \frac{1}{n} \left (E[X] + E[X] + ... + E[X] \right )\\
           &= \frac{1}{n} n E[X]\\
           &= E[X]
\end{align*}$$

Ceci prouve que $E[\bar{X}]=E[X]$.

La moyenne d'un échantillon aléatoire est donc un estimateur non biaisé de la moyenne de la population.

---

# Variance de la moyenne échantillonnale

Soit $X=\{X_1, X_2, ..., X_2\}$, un échantillon aléatoire simple de taille $n$ : 
* $\bar{X}$ la moyenne arithmétique de cet échantillon
* $\mbox{Var}(\bar{X})$ la variance échantillonnale de la moyenne :

$$\begin{align*}
\mbox{Var}[\bar{X}] &= \mbox{Var} \left [\frac{1}{n} (X_1 + X_2 + ... + X_n) \right ]\\
              &= \frac{1}{n^2} \mbox{Var} \left [(X_1 + X_2 + ... + X_n) \right ] && \mbox{(Règle 3)}
\end{align*}$$

$X$ est un échantillon aléatoire simple : les observations sont indépendantes et leur covariance est égale à zéro.

$$\mbox{Var}[\bar{X}] = \frac{1}{n^2} \left [\mbox{Var}(X_1) + \mbox{Var}(X_2) + ... + \mbox{Var}(X_n) \right ] \hspace{1in} \mbox{(Règle 4)}$$
---

# Variance de la moyenne échantillonnale

Comme l'échantillon est aléatoire, tous les membres de l'échantillon sont tirés d'une même distribution:

$$\begin{align*}
\mbox{Var}[\bar{X}] &= \frac{1}{n^2} \left [\mbox{Var}(X) + \mbox{Var}(X) + ... + \mbox{Var}(X) \right ] \\
           &= \frac{1}{n^2} n \mbox{Var}(X) \\
           &= \frac{\mbox{Var}(X)}{n}
\end{align*}$$

---

.massive[Propriétés de la régression linéaire]

---

# Calcul du coefficient de régression

Nous considérons un modèle simple avec une seule variable et une constante. 

$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$
L'indexe $i$ identifie une des $n$ observations de la banque de données.

L'objectif est de trouver les valeurs de $\beta_0$ et $\beta_1$ qui minimisent la somme des erreurs de prédiction élevées au carré:

$$\sum_{i=1}^n \varepsilon_i^2 = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2$$

---

# Calcul du coefficient de régression

Pour trouver la valeur de $\beta_0$ qui minimise les erreurs quadratiques, nous prenons la dérivée par rapport à $\beta_0$. 

À l'aide de la règle de dérivée en chaîne et du fait que la dérivée d'une somme est égale à la somme des dérivées, nous obtenons:

$$\frac{\partial \sum \varepsilon_i^2 }{\partial \beta_0} = \sum 2(\beta_0 + \beta_1 X_i - Y_i)$$

Fixer cette expression à zéro et simplifier:

$$\begin{align*}
\sum (\beta_0 + \beta_1 X_i - Y_i) = 0 \\   
\sum \beta_0 + \beta_1 \sum X_i - \sum Y_i = 0  && \mbox{(Règles 2 et 3)} \\
n\beta_0 + \beta_1 n \bar{X} - n \bar{Y} = 0 && \mbox{(Règles 1 et 4)}
\end{align*}$$

---

# Calcul du coefficient de régression

Résoudre pour $\beta_0$ donne la formule de la constante:

$$\beta_0 = \bar{Y} - \beta_1 \bar{X}$$

où $\bar{X}$ et $\bar{Y}$ sont les moyennes des deux variables.

Cette équation permet d'estimer la constante d'un modèle de régression linéaire bivarié.

---

# Calcul du coefficient de régression

Maintenant, nous dérivons la formule du coefficient de régression $\beta_1$. À l'aide de la règle de dérivée en chaîne et du fait que la dérivée d'une somme est égale à la somme des dérivées, nous obtenons:

$$\frac{\partial \sum \varepsilon_i^2 }{\partial \beta_1} = \sum 2X_i(\beta_0 + \beta_1 X_i - Y_i)$$

Remplacer $\beta_0$ par la formule de la constante $(\bar{Y} - \beta_1 \bar{X})$ donne:

$$\frac{\partial \sum \varepsilon_i^2 }{\partial \beta_1} = \sum 2X_i(\bar{Y} - \beta_1 \bar{X} + \beta_1 X_i - Y_i)$$

Fixer cette expression à zéro et simplifier:

$$\begin{align*}
\sum X_i(\bar{Y} - \beta_1 \bar{X} + \beta_1 X_i - Y_i) &= 0\\
\sum X_i\bar{Y} + \beta_1 \sum X_i(X_i - \bar{X}) - \sum X_iY_i &= 0
\end{align*}$$

---

# Calcul du coefficient de régression

Isoler $\beta_1$ donne la formule du coefficient de régression:

$$\begin{align*}
\beta_1 &= \frac{\sum X_iY_i - X_i\bar{Y}}{\sum X_i(X_i-\bar{X})} \\
        &= \frac{\sum X_i(Y_i - \bar{Y})}{\sum X_i(X_i-\bar{X})}\\
        &= \frac{\sum (X_i-\bar{X}) (Y_i-\bar{Y})}{\sum (X_i-\bar{X})^2}
\end{align*}$$

Les définitions de la variance et de la covariance nous permettent de conclure que:

$$\beta_1 = \frac{\mbox{Cov}(X, Y)}{\mbox{Var}(X)}$$

Pour calculer le coefficient de régression dans un modèle de régression bivariée, il suffit donc de diviser la covariance entre $X$ et $Y$ par la variance de $X$.

---

# Biais du coefficient de régression

La variable explicative $X$ doit être indépendante du terme résiduel $\varepsilon$ pour que le coefficient de régression linéaire soit non-biaisé. Ici, nous montrons pourquoi la condition $X\perp\varepsilon$ est si importante.

Soit:

* $\hat{\beta_1}$ est le coefficient de régression estimé dans un échantillon avec l'estimateur $(\beta_1 = \frac{\mbox{Cov}(X, Y)}{\mbox{Var}(X)})$
* $\beta_1$ est le vrai coefficient dans la population. 

L'estimateur est non biaisé s'il produit le bon résultat en moyenne: $E[\hat{\beta}_1] = \beta_1$.

---

# Biais du coefficient de régression

Pour vérifier si l'estimateur est non biaisé, nous commençons par transformer la formule du coefficient de régression:

$$\begin{align*}
\hat{\beta_1} &= \frac{\mbox{Cov}(X, Y)}{\mbox{Var}(X)}\\
              &= \frac{\sum (X_i-\bar{X})Y_i}{\sum (X_i-\bar{X})^2}
\end{align*}$$

---

# Biais du coefficient de régression

Remplacer $Y_i$ par l'équation du modèle linéaire bivarié $(\beta_0 + \beta_1 X_i + \varepsilon_i)$ et réarranger donne:

$$\begin{align*}
\hat{\beta_1} &= \frac{\sum (X_i-\bar{X})(\beta_0 + \beta_1 X_i + \varepsilon_i)}{\sum (X_i-\bar{X})^2}\\
&= \frac{\beta_0\sum (X_i-\bar{X}) + \beta_1 \sum (X_i-\bar{X})X_i + \sum (X_i-\bar{X})\varepsilon_i}{\sum (X_i-\bar{X})^2}\\
&= \frac{\beta_1 \sum (X_i-\bar{X})X_i + \sum (X_i-\bar{X})\varepsilon_i}{\sum (X_i-\bar{X})^2} \\
&= \frac{\beta_1 \sum (X_i-\bar{X})^2 + \sum (X_i-\bar{X})(\varepsilon_i-\bar{\varepsilon})}{\sum (X_i-\bar{X})^2} \\
&= \beta_1 + \frac{\mbox{Cov}(X_i, \varepsilon_i)}{\mbox{Var}(X_i)}
\end{align*}$$

---

# Biais du coefficient de régression

Puisque la vérité est fixe, $\beta_1$ est une constante, et l'espérance du coefficient estimé est:

$$\begin{align}
E[\hat{\beta_1}]
&= E\left [ \beta_1 + \frac{\mbox{Cov}(X_i, \varepsilon_i)}{\mbox{Var}(X_i)} \right ]\\
&= \beta_1 + E\left [ \frac{\mbox{Cov}(X_i, \varepsilon_i)}{\mbox{Var}(X_i)} \right ]
\end{align}$$

Si $X$ et $\varepsilon$ sont indépendants, alors en moyenne $\mbox{Cov}(X, \varepsilon)=0$, le terme de droite dans l'équation disparait, et l'estimé du coefficient est non-biaisé: $E[\hat{\beta}_1]=\beta_1$.

---

# Biais par variable omise

> Un analyste tente d'estimer l'effet de $X$ sur $Y$ sans tenir compte de la variable omise $A$:

> $$\begin{align} Y &= \beta_0 + \beta_1 X + \beta_2 A + \varepsilon && \mbox{(Modèle non biaisé)} \\ Y &= \alpha_0 + \alpha_1 X + \nu && \mbox{(Modèle biaisé)} \end{align}$$

La quantité d'intérêt est le coefficient du modèle non-biaisé $(\beta_1)$, mais l'analyste estime seulement le coefficient du modèle biaisé $(\alpha_1)$:

$$\hat{\alpha}_1 = \frac{\mbox{Cov}(Y, X)}{\mbox{Var}(X)}$$

---

# Biais par variable omise

Insérer l'équation du modèle non biaisé dans l'équation du modèle biaisé, et appliquer les règles de la covariance donne:

$$\begin{align*}
\hat{\alpha}_1
&= \frac{\mbox{Cov}(\beta_0 + \beta_1 X + \beta_2 A + \varepsilon, X)}{\mbox{Var}(X)}\\
&= \frac{\mbox{Cov}(\beta_0, X) + \mbox{Cov}(\beta_1 X, X) + \mbox{Cov}(\beta_2 A, X) + \mbox{Cov}(\varepsilon, X)}{\mbox{Var}(X)}\\
&= \frac{\mbox{Cov}(\beta_1 X, X) + \mbox{Cov}(\beta_2 A, X) + \mbox{Cov}(\varepsilon, X)}{\mbox{Var}(X)}\\
&= \frac{\beta_1 \mbox{Cov}(X, X) + \beta_2 \mbox{Cov}(A, X) + \mbox{Cov}(\varepsilon, X)}{\mbox{Var}(X)}\\
&= \beta_1 + \beta_2\cdot \frac{\mbox{Cov}(A, X)}{\mbox{Var}(X)} + \frac{\mbox{Cov}(\varepsilon, X)}{\mbox{Var}(X)}
\end{align*}$$

---

# Biais par variable omise

Nous avons postulé que le modèle $Y = \beta_0 + \beta_1 X + \beta_2 A + \varepsilon$ est non-biaisé.

Nous savons donc que $X\perp\varepsilon$, et qu'en moyenne $\mbox{Cov}(\varepsilon, X)=0$.

L'expression peut donc être simplifiée davantage:

$$\hat{\alpha}_1 = \beta_1 + \beta_2 \cdot \frac{\mbox{Cov}(A, X)}{\mbox{Var}(X)}$$

---

# Biais par variable omise

Pour interpréter cette équation, il est utile de définir un modèle de régression auxiliaire:

$$A = \pi_0 + \pi_1 X + \gamma \hspace{1in} \mbox{(Modèle auxiliaire)}$$

$\pi_1$ mesure la force de l'association entre la variable explicative et la variable omise. 

$\hat{\pi}_1$ peut être estimé par $\frac{\mbox{Cov}(A,X)}{Var(X)}$, ce qui nous donne le résultat suivant:

$$\hat{\alpha}_1 = \beta_1 + \beta_2 \cdot \pi_1$$

L'estimé biaisé $\hat{\alpha}_1$ est égal au coefficient non-biaisé $\beta_1$, plus un terme de biais égal au produit de la relation entre $A$ et $Y$ $(\beta_2)$ et de la relation entre $A$ et $X$ $(\pi_1)$.

---

# Variance du coefficient de régression

Pour dériver la formule de l'erreur type, nous appliquons l'opérateur de variance au coefficient estimé:

$$\begin{align*}
\mbox{Var}(\hat{\beta_1})
&= \mbox{Var} \left [\beta_1 + \frac{\sum (X_i-\bar{X})\varepsilon_i}{\sum (X_i-\bar{X})^2} \right ]\\
&= \mbox{Var} \left [\frac{\sum (X_i-\bar{X})\varepsilon_i}{\sum (X_i-\bar{X})^2} \right ] && \mbox{(Règle 2)}
\end{align*}$$

Puisque le modèle de régression conditionne sur les valeurs observées de $X_1,X_2,...,X_n$, nous pouvons traiter $X_i$ et $\bar{X}$ comme des constantes:

$$\mbox{Var}(\hat{\beta_1}) = \frac{1}{\left [\sum (X_i-\bar{X})^2 \right ]^2} \mbox{Var} \left [\sum (X_i-\bar{X}) \varepsilon_i \right ]$$

---

# Variance du coefficient de régression

Si les erreurs sont indépendantes les unes des autres (c.-à-d. qu'il n'y a pas d'autocorrélation), la covariance entre les erreurs de prédiction est nulle, et la variance d'une somme devient la somme des variances :

$$\begin{align*}
\mbox{Var}(\hat{\beta_1})
&= \frac{1}{\left [\sum (X_i-\bar{X})^2 \right ]^2} \sum  \mbox{Var} [(X_i-\bar{X})\varepsilon_i]\\
&= \frac{1}{\left [\sum (X_i-\bar{X})^2 \right ]^2} \sum  (X_i-\bar{X})^2 \mbox{Var} [\varepsilon_i] &&\mbox{(Règle 4)}
\end{align*}$$

Quand les erreurs sont "homoscédastiques," elles ont toutes la même variance: $\forall i \mbox{Var}(\varepsilon_i)=\sigma_\varepsilon^2$. 

---

# Variance du coefficient de régression

Si cette condition est satisfaite, nous obtenons:

$$\begin{align*}
\mbox{Var}(\hat{\beta_1})
&= \frac{1}{\left [\sum (X_i-\bar{X})^2 \right ]^2} \sum  (X_i-\bar{X})^2 \sigma_\varepsilon^2 \\
&= \frac{1}{[n \cdot \sigma^2_X ]^2} n \cdot \sigma^2_X \cdot \sigma_\varepsilon^2 \\
&= \frac{\sigma^2_\varepsilon}{n \cdot \sigma_X^2}
\end{align*}$$

L'erreur type est la racine carrée de la variance échantillonnale:

$$\sigma_{\hat{\beta}_1} = \frac{\sigma_\varepsilon}{\sqrt{n}\cdot\sigma_X}$$

---

# Conditions d'optimalité de Gauss-Markov

Le théorème Gauss-Markov identifie cinq conditions qui garantissent que la régression par les moindres carrés soit le meilleur estimateur linéaire non biaisé. 

(Par "meilleur," nous entendons qu'il s'agit de l'estimateur linéaire non biaisé ayant la plus petite variance.)

Les conditions 1 à 4 suffisent pour prouver que la régression par les moindres carrés est non biaisée. 

La condition 5 est nécessaire pour caractériser la variance des coefficients de régression.

---

## Condition 1: Le vrai modèle peut être exprimé sous forme linéaire

Le théorème Gauss-Markov assume que le vrai modèle qui génère les données peut s'exprimer par une équation comme celle-ci:

$$Y = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k + \varepsilon$$

Cette condition n'est pas aussi restrictive qu'on pourrait le penser. 

Même si le modèle doit être linéaire en ses paramètres $\beta_0, ..., \beta_k$, les variables explicatives peuvent être transformées pour s'adapter au contexte empirique.

---

## Condition 2: Le modèle est estimé à partir d'un échantillon aléatoire de la population

Cette condition restreint la méthode par laquelle les observations entrent dans l'échantillon. 

Elle n'impose pas de limite sur les relations entre les observations dans la population. 

Ce qui compte, c'est que l'échantillon soit représentatif, c'est-à-dire que tous les membres de la population aient la même probabilité d'être choisis pour faire partie de l'échantillon.

---

## Condition 3: L'espérance conditionnelle de l'erreur est égale à zéro

$$E[\varepsilon | X_1, ..., X_k] = 0$$

Cette condition est souvent présentée en deux parties: 

1. si notre modèle inclut une constante, il est raisonnable d'assumer que $E[\varepsilon]=0$.
2. si $\varepsilon \perp X_1, ..., X_k$, alors $E[\varepsilon | X_1, ..., X_k] = E[\varepsilon]=0$ (lorsque deux variables sont indépendantes, l'espérance conditionnelle est égale à l'espérance. Par conséquent)

---

## Condition 3: L'espérance conditionnelle de l'erreur est égale à zéro

Cette condition est contraignante: toutes les variables qui sont inclues dans le modèle doivent être indépendantes des variables qui déterminent $Y$, mais qui sont sont exclues du modèle, soit $\varepsilon$. 

Cette condition peut être violée si:
* le modèle linéaire est mal spécifié (p.ex. s'il manque un terme quadratique ou un logarithme). 
* le modèle souffre de biais par variable omise, de biais de sélection, de biais de mesure, ou de biais de simultanéité.

---

## Condition 4: Les variables indépendantes ne sont pas parfaitement colinéaires

Ceci est une condition technique facile à remplir. Il faut simplement éviter qu'une variable soit *parfaitement* corrélée avec une autre variable, ou avec une combinaison linéaire d'autres variables.

Par exemple, si tous les individus dans notre banque de données habitent en Amérique du Nord, il serait impossible d'inclure une variable dichotomique pour chacun des trois pays:

<center><img src='fig/tableau_21.png' width='30%'></center>

En effet, la variable binaire "Canada" est parfaitement colinéaire avec les deux autres, puisque:

$$\mbox{Canada} = 1 - \mbox{États-Unis} - \mbox{Mexique}$$

Le modèle de régression par les moindres carrés nous force à omettre une des variables dichotomiques. 

---

## Condition 5: Le terme résiduel $\varepsilon_i$ doit être "homosédastique," c'est-à-dire que sa variance conditionnelle doit être la même pour toutes les observations

$$\mbox{Var}(\varepsilon_i | X_1, ..., X_k) = \sigma^2_\varepsilon$$

---

.massive[Loi des grands nombres]

---

# Loi des grands nombres

La *loi des grands nombres* montre que lorsque la taille d'échantillons aléatoires augmente, les moyennes calculées à partir de ces échantillons convergent vers la moyenne de la population.

> Imaginez qu'une population contienne tous les nombres entiers de 0 à 100. 

> La fréquence de chaque nombre est la même, donc la moyenne de cette population est 50. 

> Piger trois nombres au hasard dans la population pourrait produire l'échantillon suivant: {2, 44, 65}.  La moyenne de cet échantillon est égale à 37. 

> Piger cinq nombres au hasard dans la population pourrait produire l'échantillon suivant: {22, 43, 46, 68, 80}. La moyenne de ce nouvel échantillon est égale à 53.8.

---

# Loi des grands nombres

<center><img src='fig/graphique_21.1.png' width='65%'></center>

---

# Loi des grands nombres

Lorsque la taille d'un échantillon augmente, sa moyenne a tendance à approcher la moyenne que nous voulons estimer dans la population.

Lorsque les échantillons sont petits, il y a beaucoup de variation entre les moyennes estimées d'un échantillon à l'autre.

Ces résultats sont absolument fondamentaux pour le champ des statistiques, et pour la méthode scientifique en général. 

Ils justifient à eux seuls l'emploi des méthodes quantitatives et l'analyse de grandes bases de données en sciences sociales. 

Plus le nombre d'individus dans notre échantillon aléatoire est grand, plus les chances sont bonnes que les caractéristiques moyennes de l'échantillon s'approchent de celles de la population.

---

.massive[Théorème central limite]

---

# Théorème central limite

Le théorème central limite montre que les moyennes d'échantillons aléatoires indépendants sont distribuées de façon (approximativement) normale lorsque la taille des échantillons est suffisamment grande.

> Notre pomicultrice pige un très grand nombre d'échantillons aléatoires composés de 10 pommes. Ensuite, elle calcule la moyenne du poids des pommes dans chacun des échantillons. Ce faisant, elle obtient un très grand nombre de moyennes échantillonnales. 

Le théorème central limite montre que ces nombreuses moyennes échantillonnales seront distribuées de façon (approximativement) normale.

Ce résultat explique pourquoi la distribution normale est si importante en statistiques. 

Lorsqu'un phénomène est le résultat de plusieurs facteurs indépendants (c.-à-d. une somme ou une moyenne), il est souvent raisonnable de croire que sa distribution est une loi normale.

---

# Théorème central limite

<center><img src='fig/tableau_21.1.png' width='60%'></center>

---

# Théorème central limite

Le théorème central limite peut être illustré à l'aide d'une simulation.

La commande `runif(10, min = 90, max = 120)` pige un échantillon aléatoire de 10 valeurs entre 90 et 120 dans une distribution uniforme:

```{r eval=FALSE}
runif(10, min = 90, max = 120)
##  [1]  106.96250 116.24819 94.08378 108.62147 113.36747 103.65861 
##  [7]  114.51243 108.91609 108.09210 105.20436
```

La commande `mean` calcule la moyenne de cet échantillon:

```{r eval=FALSE}
echantillon <- runif(10, min = 90, max = 120)
mean(echantillon)
## [1] 104.9256
```

---

# Théorème central limite

La commande `for` permet de créer un "loop" qui répète ces opérations 10000 fois, et qui enregistre les résultats dans un vecteur appelé "moyennes":

```{r eval=FALSE}
moyennes <- vector()

for (i in 1:10000) {
    echantillon <- runif(10, min = 90, max = 120)
    moyennes[i] <- mean(echantillon)
}
```

La fonction `hist` trace un histogramme des 10000 moyennes échantillonnales.

```{r eval=FALSE}
hist(moyennes)
```

---

# Théorème central limite

<center><img src='fig/graphique_21.2.png' width='60%'></center>

