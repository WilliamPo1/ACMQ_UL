---
output:
  xaringan::moon_reader:
    css: ["lib/fonts.css", "lib/slides.css"]   
    lib_dir: lib
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false
---

class: title
background-image: url(fig/wall_tomato.jpg)
background-size: cover

# ACMQ Ch.17 Modèle linéaire généralisé

---

# Modèle linéaire généralisé

Les modèles de régression linéaires sont puissants et flexibles. 

Si la vraie relation entre la variable indépendante et la variable dépendante est non linéaire, la régression par les moindres carrés risque de produire un estimé sous-optimal. 

Le GLM est une méthode flexible qui permet d'estimer un grand éventail de modèles avec différents types de variables dépendantes: continues, binaires, de dénombrement, ordinales, nominales, de durée, etc. 

Lorsque la variable dépendante n'est pas continue, le GLM produira souvent de meilleures prédictions et des estimés plus efficients que la régression linéaire.

---

.massive[Motivation]

---

# Motivation

Pour motiver l'utilisation du GLM, il est utile de considérer un modèle de régression simple avec variable dépendante binaire:

$$Y = \beta_0 + \beta_1 X + \varepsilon \hspace{1in} Y \in \{0, 1\}$$

Le coefficient de régression s'interprète comme suit: une augmentation d'une unité de la variable $X$ est associée à un changement de $100 \cdot \beta_1$ points de pourcentage dans la probabilité que $Y$ soit égale à 1. 

Ce “modèle de probabilité linéaire” est utile et facile à interpréter, mais il a deux principaux désavantages.

---

# Motivation

Premièrement, lorsque la variable dépendante est binaire, le modèle de régression linéaire peut faire des prédictions incohérentes. 

Pour certaines valeurs de $X$, le modèle linéaire prédit que la probabilité d'observer $Y=1$ est inférieure à 0, ou supérieure à 1. 

Ces prédictions violent un axiome fondamental de la théorie des probabilités.

---

# Motivation

<center><img src="fig/graphique_17.1.png" width="65%"></center>

---

# Motivation

Lorsque la relation entre $X$ et $Y$ est non linéaire, la régression linéaire peut être un estimateur inefficace. 

Le théorème Gauss-Markov prouve que la régression linéaire produit les meilleurs estimés disponibles lorsque la relation entre $X$ et $Y$ est linéaire en ses paramètres. 

Un GLM qui tient en compte la non-linéarité pourrait produire des estimés plus efficients, c'est-à-dire des estimés ayant une plus petite variance échantillonnale.

---

.massive[Le Modèle Linéaire Généralisé]

---

# Le Modèle Linéaire Généralisé 

Pour estimer un GLM il faut définir trois composantes:

1. Distribution de la variable dépendante
2. Modèle linéaire intermédiaire
3. Lien entre la distribution et le modèle intermédiaire 

En modifiant les composantes du GLM, on peut modéliser un vaste éventail de variables dépendantes. 

---

## Composante 1 : Distribution de la variable dépendante

Pour spécifier un GLM, on inspecte d'abord sa variable dépendante : 
* Si la variable dépendante est binaire, on choisit une distribution Bernoulli.
* Si la variable dépendante est une variable de dénombrement, on choisit la distribution Poisson. 
* Si la variable dépendante est continue, on choisit une distribution continue (p.ex. distribution Normale)

Après avoir choisi une distribution, on identifie le paramètre de centralité qui détermine la forme de la distribution. 

* Le paramètre $p \in [0,1]$ dicte la forme de la distribution Bernoulli.
* Le paramètre $\lambda > 0$ dicte la forme de la distribution Poisson. 
* Le paramètre $\mu$ dicte la forme de la distribution Normale.

---

## Composante 1 : Distribution de la variable dépendante

<center><img src="fig/distributions.png" width="75%"></center>

---

## Composante 2 : Modèle linéaire intermédiaire

La deuxième composante du GLM est une quantité intermédiaire dénotée $\eta$. 

Cette quantité est définie par une fonction linéaire des variables explicatives et des coefficients de régression:

$$\eta = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k$$

Cette équation n'impose pas de contrainte sur la valeur des variables explicatives ou des coefficients. 

Ceux-ci peuvent être positifs, négatifs, ou nuls, et ils peuvent être de n'importe quelle taille. 

La quantité intermédiaire $\eta$ peut assumer n'importe quel nombre réel.

---

## Composante 3 : Lien entre la distribution et le modèle intermédiaire

Cette composante fait le lien entre la quantité intermédiaire $\eta$ et le paramètre de la distribution de $Y$. 

Pour faire ce lien, on applique une fonction qui transforme $\eta$, et qui la force à prendre une valeur compatible avec le paramètre que nous tentons de modéliser.

> Si $Y$ suit une distribution Bernoulli, le paramètre $p$ doit nécessairement être contenu dans l'intervalle $[0,1]$. Nous devons trouver une fonction qui transforme $\eta$, et qui force sa valeur à l'intérieur de l'intervalle $[0,1]$. 

> Si $Y$ suit une distribution Poisson, $\lambda$ doit nécessairement être positif. Nous devons trouver une fonction qui transforme $\eta$, et qui force sa valeur à être positive. 

> Si $Y$ suit un distribution Normale, le paramètre $\mu$ peut être n'importe quel nombre réel. Nous n'avons donc pas besoin de transformer la quantité intermédiaire, puisque $\eta$ est déjà un nombre réel.

---

.massive[Exemples]

---

# Régression logistique

Le modèle de régression logistique est conçu pour modéliser les variables dépendantes binaires. 

Nous choisissons la distribution Bernoulli pour modéliser $Y$:

$$\begin{align*}
Y \sim Bernoulli(p) && (\mbox{Distribution de }Y)
\end{align*}$$

Le paramètre $p$ de la distribution Bernoulli mesure la probabilité que $Y$ soit égale à 1.

Pour modéliser ce paramètre, nous définissons d'abord une quantité intermédiaire $\eta$, fonction d'une variable explicative $X$, d'une constante $\beta_0$, et d'un coefficient $\beta_1$: 

$$\begin{align*}
\eta = \beta_0 + \beta_1 X && \mbox{(Modèle linéaire intermédiaire)}
\end{align*}$$

La quantité intermédiaire $\eta$ peut être égale à n'importe quel nombre réel. Par contre, le paramètre $p$ doit nécessairement avoir une valeur entre 0 et 1.

---

# Régression logistique

Pour faire le lien entre $\eta$ et $p$, nous appliquons la fonction logistique standard (dénotée $F$):

$$\begin{align*}
p = F(\eta) = \frac{e^\eta}{1+e^\eta},  && \mbox{(Lien)}
\end{align*}$$

où $e$ est égal à la constante de Néper 

La fonction $F$ joue le rôle de "fonction de lien inverse," en liant $\eta$ au paramètre de la distribution de $Y$.

Elle a une propriété utile: que $\eta$ soit négatif, nul, ou positif, $F(\eta)$ est toujours un nombre entre 0 et 1. 

---

# Régression logistique

<center><img src="fig/graphique_17.2.png" width="70%"></center>

---

# Régression logistique

Le paramètre $p$ représente la probabilité que la variable dépendante binaire $Y$ soit égale à 1, en fonction des variables explicatives et des coefficients. 

Nous pouvons donc représenter les prédictions du modèle de régression logistique ainsi: 

$$p = F(\beta_0 + \beta_1 X)$$

---

# Régression Poisson

Le modèle de régression Poisson est conçu pour modéliser les variables dépendantes de dénombrement (composées d'entiers non négatifs). Nous choisissons la distribution Poisson pour modéliser $Y$:

$$\begin{align*}
Y \sim Poisson(\lambda) && (\mbox{Distribution de }Y)
\end{align*}$$

Le paramètre $\lambda$ de la distribution Poisson mesure la moyenne de cette distribution.

Pour modéliser ce paramètre, nous définissons d'abord une quantité intermédiaire $\eta$, fonction d'une variable explicative $X$, d'une constante $\beta_0$, et d'un coefficient $\beta_1$: 

$$\begin{align*}
\eta = \beta_0 + \beta_1 X && \mbox{(Modèle linéaire intermédiaire)}
\end{align*}$$

La quantité intermédiaire $\eta$ peut être égale à n'importe quel nombre réel. 

Par contre, le paramètre $\lambda$ de la distribution Poisson doit nécessairement être plus grand ou égal à zéro. 

---

# Régression Poisson

Pour faire le lien entre $\eta$ et $\lambda$, nous appliquons la fonction antilogarithmique suivante:

$$\begin{align*}
\lambda = e^{\eta} && \mbox{(Lien)}
\end{align*}$$

L'exposant à base $e$ joue le rôle de "fonction de lien inverse," en liant $\eta$ au paramètre de la distribution de $Y$.^

L'exposant à base $e$ a une propriété utile: que $\eta$ soit négatif, nul, ou positif, la valeur de $e^\eta$ sera toujours plus grande ou égale à zéro. 

Le paramètre $\lambda$ représente la moyenne de la variable dépendante de dénombrement $Y$, en fonction des variables explicatives et des coefficients. 

Nous pouvons représenter les prédictions du modèle de régression Poisson ainsi:

$$\lambda = e^{\beta_0 + \beta_1 X}$$

---

# Régression Linéaire

Puisque le GLM est une généralisation de la régression linéaire, il est facile d'exprimer un modèle analogue à la régression linéaire par les moindres carrés dans son cadre théorique. 

La variable dépendante est continue, et elle peut prendre des valeurs dans l'ensemble des nombres réels : nous choisissons la distribution Normale pour modéliser $Y$: 

$$\begin{align*}
Y \sim Normale(\mu, \sigma^2) && (\mbox{Distribution de} Y)
\end{align*}$$

Le paramètre $\mu$ de la distribution normale mesure la moyenne de cette distribution.

Pour modéliser ce paramètre, nous définissons d'abord une quantité intermédiaire $\eta$, fonction d'une variable explicative $X$, d'une constante $\beta_0$, et d'un coefficient $\beta_1$: 

$$\begin{align*}
\eta = \beta_0 + \beta_1 X && \mbox{(Modèle linéaire intermédiaire)}
\end{align*}$$

---

# Régression Linéaire

La quantité intermédiaire $\eta$ peut être égale à n'importe quel nombre réel. 

Le paramètre $\mu$ de la distribution peut aussi être égal à n'importe quel nombre réel. Par conséquent, nous n'avons pas besoin de transformer $\eta$. 

Le lien peut simplement affirmer l'identité des deux quantités:

$$\begin{align}
\mu = \eta && \mbox{(Lien)}
\end{align}$$

Le paramètre $\mu$ représente la moyenne de $Y$, en fonction des variables explicatives et des coefficients. 

Puisque $\mu=\eta$, nous pouvons passer directement de $\mu$ à l'équation linéaire, et représenter les prédictions du modèle ainsi:

$$\mu = \beta_0 + \beta_1 X$$

---

.massive[Estimation, postulats et propriétés]

---

# Estimation, postulats et propriétés

Les logiciels statistiques estiment le GLM à l'aide d'une technique appelée le "maximum de vraisemblance". 

Cette méthode nous force à accepter deux postulats pour garantir que les coefficients estimés soient adéquats: 
1. les observations doivent être indépendantes les unes des autres, et
2. elles doivent toutes être issues d'une seule distribution.

Le GLM estimé par maximum de vraisemblance a plusieurs propriétés attrayantes. 

Lorsque la taille de l'échantillon tend vers l'infini, les estimés produits sont efficients, consistants, et distribués normalement.

---

.massive[Variable binaire: Régression logistique]

---

# Variable binaire: Régression logistique

Revisitons les déterminants de la survie à bord du Titanic à l'aide d'un modèle de régression logistique. 

Dans ce modèle, la probabilité de survie d'un individu dépend de son âge et de son genre:

$$P(S=1) = F(\beta_0 + \beta_1 \cdot A + \beta_2 \cdot G)$$

$S$ est une variable binaire égale à 1 si l'individu a survécu, et 0 sinon

$F$ est la fonction logistique standard

$A$ est une variable continue égale à l'âge du passager 

$G$ est une variable qui représente le genre, égale à 1 pour les femmes et 0 pour les hommes

---

# Variable binaire: Régression logistique

Nous estimons le modèle avec la fonction `glm`:

```{r eval=FALSE}
dat <- read.csv('data/titanic.csv')
mod <- glm(survie ~ age + femme, family = binomial('logit'), 
           data = dat)
summary(mod)
##
##              Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.159839   0.219651  -5.280 1.29e-07 ***
## age         -0.006352   0.006187  -1.027    0.305    
## femme        2.465996   0.178455  13.819  < 2e-16 ***
```

Le coefficient pour `age` est négatif : la probabilité de survie des aînés est plus faible que la celle des jeunes. 

Le coefficient pour `femme` est positif : la probabilité de survie des femmes est plus élevée que celle des hommes.

---

# Variable binaire: Régression logistique

Pour interpréter les résultats de régression logistique, il est utile de considérer trois quantités : 
1. la prédiction, 
2. l’effet marginal, et 
3. le rapport des cotes.

---

## 1. Prédiction

Une première quantité d'intérêt dans le modèle de régression logistique est la probabilité que la variable dépendante soit égale à 1. 

Pour prédire cette probabilité, il suffit d'insérer les coefficients et les valeurs de nos variables dans l'équation.

Par exemple, pour prédire la probabilité de survie d'une femme de 25 ans, nous calculons:

$$\begin{align*}
P(S=1) &= F(\beta_0 + \beta_1 \cdot A + \beta_2 \cdot G)\\
                   &= F(-1,1598 - 0,0064 \cdot A + 2,4660 \cdot G)\\
                   &= F(-1,1598 - 0,0064 \cdot 25 + 2,4660 \cdot 1) \end{align*}$$

---

## 1. Prédiction

Dans `R` la commande `plogis` permet d'appliquer la fonction logistique $F$:

```{r eval=FALSE}
plogis(-1.1598 - 0.0064 * 25 + 2.4660 * 1)
```

La librairie `prediction` permet d'obtenir le même résultat plus facilement:

```{r eval=FALSE}
library(prediction)
prediction(mod, at = list('femme' = 1, 'age' = 25))
##  femme age     x
##      1  25 0.759
```

Notre modèle prédit que la probabilité de survie d'une femme de 25 ans est de 75,9%. 

---

## 2. Effet marginal

Une deuxième quantité d'intérêt dans le modèle de régression logistique est l'effet marginal, c.-à-d. la dérivée partielle de l'équation de régression par rapport à la variable explicative.

L'effet marginal mesure l'effet de la variable indépendante sur la variable dépendante, *ceteris paribus*.

Dans un modèle de régression linéaire simple, l'effet marginal correspond au coefficient de régression: $\frac{\partial Y}{\partial X}=\beta_1$. 

> Ce coefficient est facile à interpréter: une augmentation d'une unité de $X$ est associée à un changement de $\beta_1$ unités dans $Y$.

Dans un GLM non linéaire, les coefficients de régression ne correspondent pas directement à l'effet marginal. 

On ne peut *pas* les interpréter directement, comme s'ils mesuraient la force de l'association entre $X$ et $Y$. 

---

## 2. Effet marginal

Pour identifier l'effet marginal, on prend d'abord la dérivée partielle de l'équation de régression. 

En déployant les règles du calcul différentiel, on peut montrer que la dérivée partielle par rapport à $A$ est:

$$\frac{\partial P(S=1)}{\partial A} = \beta_1 \cdot f(\beta_0 + \beta_1 A + \beta_2 G),$$

où $f$ représente la "fonction de densité de la loi de distribution logistique," ou la dérivée de la fonction logistique $F$.

Cette équation révèle deux idées importantes: 

1. Dans un GLM non linéaire, l'effet marginal d'une variable explicative dépend généralement des valeurs de tous les coefficients du modèle, et de toutes les variables explicatives.
2. Puisque les variables explicatives varient d'une observation à l'autre, chaque observation est associée à un effet marginal propre.

---

## 2. Effet marginal
 
Pour calculer l'effet marginal de l'âge pour une observation donnée, il faut insérer les coefficients estimés et les caractéristiques de l'observation dans l'équation. 

Par exemple, l'effet marginal de l'âge sur la probabilité de survie d'un homme de 58 ans est égal à:

$$\begin{align*}
\frac{\partial P(S=1)}{\partial A} 
&= \beta_1 \cdot f(\beta_0 + \beta_1 \cdot A + \beta_2 \cdot G) \\
&= -0,0064 \cdot f(-1,1598 -0,0064 \cdot A + 2,4660 \cdot G) \\
&= -0,0064 \cdot f(-1,1598 -0,0064 \cdot 58 + 2,4660 \cdot 0)
\end{align*}$$

---

## 2. Effet marginal

Dans `R`, la commande `dlogis` permet d'appliquer la distribution logistique $f$:

```{r eval=FALSE}
-0.0064 * dlogis(-1.1598 + (-0.0064) * 58 + 2.4660 * 0)
## [1] -0.0009357934
```

La librairie `margins` permet d'obtenir le même résultat plus facilement:

```{r eval=FALSE}
library(margins)
margins(mod, variables = 'age', 
        at = list('femme' = 0, 'age' = 58))
##  at(femme) at(age)        age
##          0      58 -0.0009304
```

Notre modèle estime que pour un homme de 58 ans, vieillir d'un an serait associé à un changement de -0,09 point de pourcentage dans la probabilité de survie. 

---

## 2. Effet marginal

<center><img src="fig/tableau_17.1.png" width="50%"></center>

---

## 2. Effet marginal

L'effet marginal varie d'un individu à l'autre : il est souvent plus simple de se concentrer sur l'effet marginal moyen. 

Pour obtenir cette quantité, nous calculons l'effet marginal pour tous les individus de notre banque de données, et nous prenons la moyenne des effets marginaux individuels.

La librairie `margins` fait ce travail pour nous:

```{r eval=FALSE}
mfx <- margins(mod, variables = 'age')
summary(mfx)
##  factor     AME     SE       z      p   lower  upper
##     age -0.0011 0.0011 -1.0283 0.3038 -0.0032 0.0010
```

`AME` est le "Average Marginal Effect," ou l'effet marginal moyen.

Est-ce que la relation entre l'âge et la survie est forte? Non.

---

## 2. Effet marginal *d'une variable binaire*

Comme l'effet marginal est défini par la dérivée partielle, l'interprétation donnée ci-haut est valide seulement pour un petit changement dans une variable explicative continue. 

Pour interpréter l'effet d'une variable dichotomique, il faut comparer les probabilités prédites par le modèle. 

La librairie `prediction` permet de comparer les probabilités de survie des hommes et des femmes de l'échantillon:

```{r eval=FALSE}
prediction(mod, at = list('femme' = c(0, 1)))
##   femme      x
##       0 0.2058
##       1 0.7523
```

La probabilité de survie moyenne pour un homme est de 20,58% ; pour une femme moyenne, 75,23%.

L’effet marginal de `femme` est égal à la différence entre ces deux quantités: 75,23 - 20,58 = 54,65.

---

## 3. Rapport des cotes

La troisième quantité qui permet d'interpréter la régression logistique s'appelle le "rapport des cotes" ("odds ratio"). 

Une cote est définie par le ratio suivant:

$$\mbox{Cote} = \frac{P(Y=1|X)}{1 - P(Y=1|X)}$$

Si la probabilité de survie d'un homme de 25 ans est égale 21,1045%, sa cote est égale à: 

$$\mbox{Cote}_{G=0,A=25} = \frac{0,211045}{1-0,211045}=0,2674994$$

Si la probabilité de survie d'une femme de 25 ans est de 75,9028%, sa cote est égale à: 

$$\mbox{Cote}_{G=1,A=25} = \frac{0,759028}{1-0,759028}=3,14986$$

---

## 3. Rapport des cotes

Une cote est souvent interprétée en termes de "chances" : plus la cote d'un individu est élevée, plus la "chance" que la variable dépendante soit égale à 1 est élevée. 

Il est important de ne pas confondre "chances" et "probabilités", puisque la chance est un ratio de probabilités.

Le rapport des cotes est défini comme le ratio de deux cotes:

$$\frac{\mbox{Cote}_{G=1,A=25}}{\mbox{Cote}_{G=0,A=25}} = \frac{3,14986}{0,2674994} = 11.7752$$

Ce rapport des cotes montre que la cote d'un homme qui voyage à bord du Titanic serait près de 12 fois plus grande si ce passager était une femme.

En contraste, un rapport de cote plus petit que 1 signifierait que les chances de survie sont plus petites lorsque la variable explicative augmente.

---

## 3. Rapport des cotes

Le rapport des cotes est intimement lié aux coefficients du modèle de régression logistique.

Le coefficient de régression associé à la variable de genre est de:
 
```{r eval=FALSE}
coef(mod)['femme']
##    femme 
## 2.465996 
```

Élever la constante $e$ à la puissance du coefficient $(e^{\beta_2})$ produit le même rapport des cotes calculé précédemment:

```{r eval=FALSE}
exp(2.465996)
## [1] 11.7752
```

Ce résultat signifie qu'une augmentation de 1 unité sur la variable explicative $G$ multiplie par environ 12 la cote. 

Le fait d'être une femme multiplie par environ 12 les chances relatives de survie d'un passager.

---

## 3. Rapport des cotes

Le rapport des cotes a deux principaux désavantages. 

1. Il est difficile à interpréter correctement : le concept de "cote" est défini comme un ratio de probabilités, et le rapport des cotes est un ratio de ratios. 

2. Comme il s'agit d'une mesure des chances *relatives*, il ne communique pas toute l'information pertinente sur le changement *absolu* du risque. 

> Une personne qui achète deux billets de loto a deux fois plus de chances de gagner, mais à toutes fins pratiques, ses chances de gagner demeurent les mêmes que si elle n'avait qu'un seul billet.

---

## 3. Rapport des cotes

La distinction entre changement *relatif* ou *absolu* peut être illustrée par un exemple numérique. 

> Un chercheur s'intéresse à l'effet d'un gène sur une maladie dégénérative rare.  Il estime que la probabilité d'être affecté par cette maladie est égale à : 
> * 0,1% pour les individus qui ont le gène $(X=1)$
> * 0,01% pour les individus qui n'ont pas le gène $(X=0)$. 

Dans ce cas, les cotes sont:

$$\begin{align*}
\mbox{Cote}_{X=0} = \frac{0,0001}{1-0,0001}=0,00010001\\
\mbox{Cote}_{X=1} = \frac{0,001}{1-0,001}=0,001001
\end{align*}$$

---

## 3. Rapport des cotes

Le rapport des cotes est égal à:

$$\frac{\mbox{Cote}_{X=1}}{\mbox{Cote}_{X=0}} = \frac{0,001001}{0,00010001} \approx 10$$

Ce rapport des cotes est très grand (environ 10). 

Par contre, la probabilité de souffrir de la maladie est seulement 0,09 point de pourcentage plus grande chez les individus qui ont le gène par rapport à ceux qui ne l'ont pas. 

Le rapport des cotes ne communique pas l'information pertinente sur le risque de base.

---

.massive[ Variable de dénombrement: Régression Poisson]

---

# Variable de dénombrement: Régression Poisson

Le modèle de régression Poisson est conçu pour analyser les variables dépendantes de dénombrement (c.-à-d. nombres entiers non négatifs). 

De façon générique, ce modèle peut être exprimé ainsi:

$$\lambda = e^{\beta_0 + \beta_1 X + \beta_2 Z}$$

Pour illustrer l’estimation d’un modèle de régression Poisson, nous allons étudier les données analysées par Fair (1978) dans son article "*A Theory of Extramarital Affairs*."

---

# Variable de dénombrement: Régression Poisson

```{r eval=FALSE}
dat <- read.csv('data/aventures.csv')
head(dat)
##   aventures enfants heureux
## 1         0       0       1
## 2         0       0       1
## 3         3       0       1
## 4         0       1       1
## 5         3       1       0
## 6         0       1       0
```

`aventures` mesure le nombre d'aventures extra-conjugales rapportées par un répondant au cours d'une année

`enfants` est une variable binaire égale à 1 si le répondant a au moins un enfant

`heureux` est une variable binaire égale à 1 si le répondant considère que son mariage est heureux. 

---

# Variable de dénombrement: Régression Poisson

La fonction `glm` permet d'estimer un modèle de régression Poisson :

```{r eval=FALSE}
mod <- glm(aventures ~ enfants + heureux, 
           family = poisson(), data = dat)
summary(mod)
##
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)
## (Intercept) -0.04553    0.08142  -0.559  0.57604
## enfants      0.63136    0.08873   7.116 1.11e-12
## heureux     -0.21485    0.07482  -2.872  0.00408
```

Le coefficient associé à la variable `enfant` est positif. 

Le coefficient associé à la variable `heureux` est négatif.

---

# Prédiction

$$\lambda = e^{\beta_0 + \beta_1 X + \beta_2 Z}$$

Pour calculer la valeur prédite de la variable dépendante pour un individu donné, il suffit d'insérer les coefficients estimés et les caractéristiques de l'individu dans l'équation.

Nous pouvons utiliser la librairie `prediction`. Par exemple, le nombre prédit d'aventures extra-conjugales pour une personne heureuse sans enfant est:

```{r eval=FALSE}
library(prediction)
prediction(mod, at = list(enfants = 0, heureux = 1))
##  enfants heureux      x
##        0       1 0.7708
```

En moyenne, le nombre d'aventures extra-conjugales pour ce type de personne est donc inférieur à 1.

---

# Effet marginal

$$\lambda = e^{\beta_0 + \beta_1 X + \beta_2 Z}$$

L'effet marginal mesure la force de l'association entre $X$ et $Y$, toutes choses étant égales par ailleurs. 

Pour mesurer l'effet marginal d'une variable continue, il faut prendre la dérivée partielle de l'équation de régression par rapport à la variable explicative d'intêret. 

En appliquant les règles du calcul différentiel, il est possible de montrer que la dérivée partielle du modèle Poisson par rapport à $X$ (l'effet marginal de $X$ sur $\lambda$) est égal à:

$$\frac{\partial \lambda}{\partial X} = \beta_1 \cdot e^{\beta_0 + \beta_1 X + \beta_2 Z}$$

---

# Effet marginal

Dans le cas des aventures extra-conjugales, les variables explicatives sont binaires. 

Il est préférable de comparer la prédiction du modèle lorsque "enfants" est égal à 1 à la prédiction du modèle lorsque "enfants" est égal à 0:

```{r eval=FALSE}
prediction(mod, at = list('enfants' = c(0, 1)))
##  enfants      x
##        0 0.8959
##        1 1.6844
```

En moyenne, être parent augmente de $1,68 - 0,90 = 0,79$ le nombre d’aventures extra-conjugales.

---

# Effet multiplicatif

$$\lambda = e^{\beta_0 + \beta_1 X + \beta_2 Z}$$

La fonction de lien utilisée pour spécifier le modèle Poisson ouvre une autre possibilité pour son interprétation. 

Une augmentation de 1 unité de la variable $X$ multiplie par $e^{\beta_1}$ la valeur prédite de $Y$.

Par exemple, passer de 0 à 1 sur la variable "enfants" multiple par $e^{0,063} = 1,88$ le nombre d'aventures extra-conjugales prédites par le modèle.

---

.massive[Autres types de variables dépendantes]

---

# Autres types de variables dépendantes

Le GLM est une généralisation du modèle de régression linéaire qui a permis d'estimer deux types de modèles différents, adaptés aux variables dépendantes binaires ou de dénombrement. 

Le cadre conceptuel du GLM permet d'estimer beaucoup d'autres modèles : 
* les variables binaires $\Longrightarrow$ GLM Probit; 
* les variables ordinales $\Longrightarrow$ modèle probit ou logit ordinal; 
* les variables nominales $\Longrightarrow$ modèle multinomial probit ou logit;
* les variables de durée avec censure $\Longrightarrow$ modèle de survie (p.ex. la régression de Cox); 
* les variables censurées $\Longrightarrow$ modèle Tobit. 
